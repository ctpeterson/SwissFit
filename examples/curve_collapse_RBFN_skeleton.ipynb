{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0700fec0-6bb6-4749-a69f-ccb68da085fd",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Let's say you want to do a curve collapse analsis of an observable $O$ by fitting data to one of the following fit ansatz:\n",
    "$$O(K,N_{\\mathrm{s}})=N_{\\mathrm{s}}^{\\gamma_{O}}\\mathcal{F}_{O}\\Big((K/K_{\\mathrm{c}}-1)N_{\\mathrm{s}}^{1/\\nu}\\Big)$$\n",
    "for 2nd-order or\n",
    "$$O(K,N_{\\mathrm{s}})=N_{\\mathrm{s}}^{\\gamma_{O}}\\mathcal{F}_{O}\\Big(N_{\\mathrm{s}}\\exp\\big(\\mbox{-}\\zeta |K/K_{\\mathrm{c}}-1|^{-\\nu}\\big)\\Big)$$\n",
    "for $\\infty$-order (BKT). This notebook will guide you through how do to this analysis. To see how to scale this up to a full empirical Bayes analysis, see `examples/potts2_example.ipynb`, `examples/potts3_example.ipynb`, `examples/clock4_example.ipynb` or `examples/clockinf_example.ipynb`. \n",
    "\n",
    "# Import modules\n",
    "We need to load up some modules. First, let's import GVar and Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0404acd-49c5-4faa-8059-308113293209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gvar as gv # Peter Lepage's GVar module\n",
    "import numpy as np # NumPy for number crunching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaeef7d-86ce-41c8-b74e-ecd8d28512d5",
   "metadata": {},
   "source": [
    "Next, we import all of SwissFit's modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6caad741-1d0b-4587-997b-10d6c30a649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from swissfit import fit # SwissFit fitter module\n",
    "from swissfit.optimizers import scipy_basin_hopping # Basin hopping global optimizer\n",
    "from swissfit.optimizers import scipy_least_squares # Trust region reflective local optimizer\n",
    "from swissfit.machine_learning import radial_basis # Module for radial basis function network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f4ea04-e55c-4364-9c1b-37d9e31a0298",
   "metadata": {},
   "source": [
    "# Grab your data\n",
    "For the curve collapse analysis, our input data is a list of pairs $(K,N_{\\mathrm{s}})$ and our output data is a list of measurements $O(K,N_{\\mathrm{s}})$. You need to write your own code to grab the data that you want to do your analysis with. `SwissFit` will take this data in as a dictionary that is organized as \n",
    "```\n",
    "data = {'x': [[K_1, N_2], [K_2, N_2], ...], 'y': [O_1, O_2, ...]}\n",
    "```\n",
    "where `data['x']=[[K_1, N_2], [K_2, N_2], ...]` is a list of $(K,N_{\\mathrm{s}})$ pairs and `data['y']=[O_1, O_2, ...]` is a list of `GVar` variables for $O(K,N_{\\mathrm{s}})$. Here's a skeleton of how you might do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9a460b-3a75-4e13-9e9b-4aa2f7bdb559",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Insert your code that grabs your list of (K,N) pairs [[K_1, N_2], [K_2, N_2], ...] \n",
    "and output GVar variables [O_1, O_2, ...] here. Below, your input pairs are saved into\n",
    "\"input\" and your output variables are saved into \"output\" as an example.\n",
    "\"\"\"\n",
    "\n",
    "# Input data\n",
    "input = # [[K_1, N_1], [K_2, N_2], ...]\n",
    "\n",
    "# Output data\n",
    "output = # [O_1, O_2, ....]\n",
    "\n",
    "# Save your data as a dictionary (see above for description)\n",
    "data = {'x': input, 'y': output}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c2925b-b852-45e4-b280-8a35b4bd2725",
   "metadata": {},
   "source": [
    "# Set up radial basis function network\n",
    "Now we need to create our radial basis function network (RBFN). This proceeds in two steps. First, we create a dictionary that defines the topology of the RBFN. The following network will have a single hidden layer with two nodes in that layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab405197-d811-4d44-8ce6-15b33af9e83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the number of hidden nodes\n",
    "hidden_nodes = 2 # *** Change if you want more hidden nodes ***\n",
    "\n",
    "# Specify network topology\n",
    "network_topology = {\n",
    "    'lyr1': { # Hidden layer\n",
    "        'in': 1, 'out': hidden_nodes, \n",
    "        'activation': 'exp', # Exponential activation\n",
    "    },\n",
    "    'lyr2': { # Output layer\n",
    "        'in': hidden_nodes, 'out': 1,\n",
    "        'activation': 'linear' # Linear activation\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689bb307-e50d-4420-886b-5109e1ae9e83",
   "metadata": {},
   "source": [
    "Now we create the RBFN by simply passing `network_topology` through the RBFN constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f300d3de-26ba-410b-a19b-88f5c1dc34d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create radial basis function network\n",
    "neural_network = radial_basis.RadialBasisNeuralNetwork(network_topology)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376395fb-9ec4-4965-8e11-8be3880f6dc1",
   "metadata": {},
   "source": [
    "# Define your fit function\n",
    "Now that we have the radial basis function network set up, let's define the fit function. For 2nd-order scaling, \n",
    "$$\\mathrm{fit\\_function}(K,N_{\\mathrm{s}})=N_{\\mathrm{s}}^{\\gamma_{O}}\\mathcal{F}_{O}\\Big((K/K_{\\mathrm{c}}-1)N_{\\mathrm{s}}^{1/\\nu}\\Big),$$\n",
    "this could be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a5c11a-583e-45e8-8e63-2bd9c47e6675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd-order scaling\n",
    "def fit_fcn_2nd_order(b, l, p):\n",
    "    # This is the scaling function parameterized by the neural network\n",
    "    F_O = np.ravel(neural_network.out((b * p['c'][0] - 1.) * l**p['c'][1], p))\n",
    "\n",
    "    # This is N_s**gamma_O * F_O\n",
    "    return F_O * l**p['c'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b3fbb0-42e6-411d-95a1-d8d5c7d746b8",
   "metadata": {},
   "source": [
    "If the operator that you are measuring has $\\gamma_{O}=0$, just don't multiply `F_O` by `l**p['c'][-1]`. In the above example, you'll see that the critical parameters are specified in a dictionary `p` with \n",
    "```\n",
    "p['c'] = [<1/K_c>, <1/nu>, <gamma_O>].\n",
    "```\n",
    "The parameters of the RBFN are contained in other dictionary entires of `p`.\n",
    "\n",
    "Infinite-order scaling, \n",
    "$$\\mathrm{fit\\_function}(K,N_{\\mathrm{s}})=N_{\\mathrm{s}}^{\\gamma_{O}}\\mathcal{F}_{O}\\Big(N_{\\mathrm{s}}\\exp\\big(\\mbox{-}\\zeta |K/K_{\\mathrm{c}}-1|^{-\\nu}\\big)\\Big),$$\n",
    "is specified in a similar way as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298f0871-2645-4a24-a104-4fba8dd2de63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infinite-order (BKT) scaling\n",
    "def fit_fcn_infinite_order(b, l, p):\n",
    "    # This is the scaling function parameterized by the neural network\n",
    "    F_O = np.ravel(neural_network.out(\n",
    "        l / gv.exp(p['c'][2] * gv.abs(b * p['c'][0] - 1.)**(-p['c'][1])), # Argument of F_O\n",
    "        p # Parameters of the RBFN\n",
    "    ))\n",
    "\n",
    "    # This is N_s**gamma_O * F_O\n",
    "    return F_O * l**p['c'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40319dbf-b261-4c01-bada-01a13a3fd2d7",
   "metadata": {},
   "source": [
    "Again, you'll notice that the critical parameters are contained in a dictionary `p`, with\n",
    "```\n",
    "p['c'] = [<1/K_c>, <nu>, <zeta>, <gamma_O>]\n",
    "```\n",
    "and the RBFN parameters contained as other entries of the `p` dictionary.\n",
    "\n",
    "# Set priors and starting values for parameters\n",
    "Now that we have our fit function, we need to get our priors. You can do this any way that you want, as long as everything is in a dictionary that `SwissFit` accepts. Take the code below as a template. You can also just not specify any priors if you don't feel like it. It's up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcb2de8-413e-410a-bde7-c0878e6e9a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your dictionary of priors\n",
    "prior = {}\n",
    "\n",
    "# Specify priors\n",
    "prior['c'] = # [fill with GVar variables corresponding to the priors for the critical paramters]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a5180c-2763-4c70-951f-697a6d36a8ae",
   "metadata": {},
   "source": [
    "If you also want to specify ridge regression priors for the RBFN, the code is as follows. Note that *this is optional*. You don't need to specify a ridge regression prior for the network, but it can (and often does) help prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc6e31-ea17-4e14-8962-72602a3f9461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean/width of ridge regression prior\n",
    "ridge_mean = 0. # Mean\n",
    "ridge_width = 100. # Width - tune to a smaller value if you are overfitting\n",
    "\n",
    "# Specify ridge regression prior.\n",
    "prior = neural_network.network_priors(\n",
    "    prior_choice_weight = { # Prior for weights\n",
    "        'lyr2': { # Only for output layer\n",
    "            'prior_type': 'ridge_regression', # Type of prior\n",
    "            'mean': ridge_mean, # Mean of zero\n",
    "            'standard_deviation': ridge_width # Width of lambda\n",
    "        }\n",
    "    }, \n",
    "    prior = prior # Take in already-specified prior dictionary and modify it\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea49b79-bf43-4fc1-b113-31d6b436e1b0",
   "metadata": {},
   "source": [
    "Your fit also needs a starting point. Below is a skeleton of how to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befd58c6-feb5-4c4f-9ebb-05b774fac924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionary of starting values\n",
    "p0 = {}\n",
    "\n",
    "# Initialize critical parameters (if you already specified these as priors, you don't have to do this)\n",
    "p0['c'] = # [fill with starting values for critical parameters]\n",
    "\n",
    "# Initialize the parameters of the radial basis function network\n",
    "p0 = neural_network.initialize_parameters(initialization = 'zero', p0 = p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b9fba7-3a48-42fd-bc93-5a77420ac105",
   "metadata": {},
   "source": [
    "# Set up fitter\n",
    "Now we will create the `SwissFit` fitter `fitter`. The `fitter` is `SwissFit`'s analogue of `Lsqfit`'s `nonlinear_fit`. The `fitter` will optionally take in a function called `prior_transformation_function` that takes the priors and transforms them into log priors, which forces whatever parameters that you want to be positivite. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08980bb6-62fc-4ab0-ab57-c2d08cca7c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that transforms the priors into log priors to force positivity on critical parameters *** OPTIONAL ***\n",
    "log_priors = {'c': lambda x: gv.log(x)} # *** OPTIONAL ***\n",
    "\n",
    "# Create SwissFit fitter\n",
    "fitter = fit.SwissFit(\n",
    "    udata = data, # Fit data; \"data = data\" is also acceptable - \"udata\" means \"uncorrelated\"\n",
    "    uprior = prior, # Priors; \"prior = prior\" is also acceptable - \"uprior\" means \"uncorrelated\"\n",
    "    p0 = p0, # Starting values for parameters\n",
    "    fit_fcn = fit_fcn, # Fit function\n",
    "    prior_transformation_fcn = log_priors # Transformation of prior \"c\" to \"log(c)\" *** OPTIONAL ***\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa23bbc4-efb0-424e-8e69-636821304bfd",
   "metadata": {},
   "source": [
    "# Set up the optimization algorithms\n",
    "Now we need to set up the optimization algorithms. First, let's create an object that represents the trust region reflective algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35be292-0f7f-43d7-beb8-9c228ba51932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trust region reflective local optimizer from SciPy - fitter will save reference to local_optimizer for basin hopping\n",
    "local_optimizer = scipy_least_squares.SciPyLeastSquares(fitter = fitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c81fb12-bd94-41da-ad9d-346590509512",
   "metadata": {},
   "source": [
    "Now that we've set up the local optimization algorithm, we can create our basin hopping global optimization algorithm as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d09eca-24a7-4481-a57b-a066f78846e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basin hopping parameters\n",
    "niter_success = 100 # Number of iterations with same best fit parameters for basin hopping to \"converge\"\n",
    "niter = 10000 # Upper bound on total number of basin hopping iterations\n",
    "T = 1. # Temperature hyperparameter for basin hopping\n",
    "\n",
    "# Basin hopping global optimizer object instantiation\n",
    "global_optimizer = scipy_basin_hopping.BasinHopping(\n",
    "    fitter = fitter, # Fit function is the \"calculate_residual\" method of fitter object\n",
    "    optimizer_arguments = {\n",
    "        'niter_success': niter_success,\n",
    "        'niter': niter,\n",
    "        'T': T,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc080372-73ac-4243-a57c-22b972d489e2",
   "metadata": {},
   "source": [
    "# Do fit\n",
    "Now that everything is set up, doing the fit is a single line of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531515ff-1306-4ed2-8eec-c3d472fc0172",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter(global_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45bbb00-0689-44b5-9d84-fd4e0b7f8b69",
   "metadata": {},
   "source": [
    "If you want to print out the result of the fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be123193-9626-4f25-9d65-be2bc23be06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3831d93-d964-4d40-be65-d5ed3a97482a",
   "metadata": {},
   "source": [
    "If you want to grab your (correlated) fit parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9234e5bb-53be-4e2d-bba7-6fe9a8f7ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_parameters = fitter.p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a78c70-a18f-400b-a739-c8ca3740e1a8",
   "metadata": {},
   "source": [
    "If you want to get the value of your fit at a specific $(K,N_{\\mathrm{s}})$:\n",
    "```\n",
    "value = your_fit_function(K, Ns, fit_parameters)\n",
    "```\n",
    "where `your_fit_function` is either `fit_fcn_2nd_order` or `fit_fcn_infinite_order`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
