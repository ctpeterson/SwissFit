{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90d68205-e310-4104-b755-1d67bbeae8da",
   "metadata": {},
   "source": [
    "In this notebook, I compare the output of `SwissFit` against the output of Peter Lapage's [lsqfit](https://github.com/gplepage/lsqfit) on an already-optimized set of starting parameters for a curve collapse analysis of the Ising model. This goal of this comparison is to pick out any differences in the estimate of important statistical quantities from both codes. First, let's load up the appropriate modules and grab both the staring parameters and fit data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98be5e14-2535-4f60-8a30-0ad6124a6000",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" External modules \"\"\"\n",
    "import gvar as gv # Peter Lepage's GVar module\n",
    "import lsqfit as lsqfit # Peter Lepage's lsqfit module\n",
    "import numpy as np # NumPy for number crunching\n",
    "\n",
    "\"\"\" Local modules \"\"\"\n",
    "import example_tools # For getting example data\n",
    "\n",
    "\"\"\" SwissFit modules \"\"\"\n",
    "from swissfit import fit # SwissFit fitter module\n",
    "from swissfit.optimizers import scipy_least_squares # Trust region reflective local optimizer\n",
    "from swissfit.machine_learning import radial_basis # Module for radial basis function network\n",
    "\n",
    "\"\"\" Grab data \"\"\"\n",
    "Kl, Kh = 1. / 2.3, 1. / 2.22 # K = J/T, where T is the standard Ising temp.\n",
    "volumes = ['64', '96', '128', '256'] # Ns values\n",
    "data, starting_parameters = example_tools.potts2_data('u', Kl, Kh, volumes)\n",
    "p0 = gv.mean(starting_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d380b413-7add-404f-b11b-e0d45cf7ed01",
   "metadata": {},
   "source": [
    "Now let's set up the neural network and fit function as done in the `examples/potts2_example.ipynb` notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b314d98-87f1-4b31-84df-ae0f53d5b78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define radial basis function network topology\n",
    "network_topology = {\n",
    "    'lyr1': { # Hidden layer\n",
    "        'in': 1, 'out': 2, # Dimension of input & output\n",
    "        'activation': 'exp', # Exponential activation\n",
    "    },\n",
    "    'lyr2': { # Output layer\n",
    "        'in': 2, 'out': 1,\n",
    "        'activation': 'linear' # Linear activation\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create radial basis function network\n",
    "neural_network = radial_basis.RadialBasisNeuralNetwork(network_topology)\n",
    "\n",
    "# Define fit function for SwissFit\n",
    "def fit_fcn_swissfit(b, l, p):\n",
    "    return np.ravel(neural_network.out((b * p['c'][0] - 1.) * l**p['c'][1], p))\n",
    "\n",
    "# Define fit function for lsqfit\n",
    "def fit_fcn_lsqfit(p): return fit_fcn_swissfit(*np.transpose(data['x']), p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97356fa7-47ea-4e3b-82ba-339de8bccbb9",
   "metadata": {},
   "source": [
    "Finally, let's go ahead and grab priors for both codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e85385aa-0a97-44de-bf26-3318516b9cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function that grabs the fit parameters\n",
    "def get_priors(code, lmbda_weight, lmbda_center, lmbda_bandwidth, lmbda_bias):\n",
    "    # Initialize dictionary of priors\n",
    "    prior = {}\n",
    "    \n",
    "    # Prior on critical parameters K_c & 1/nu\n",
    "    if code == 'swissfit': prior['c'] = [gv.gvar('2.0(2.0)'), gv.gvar('1.0(1.0)')]\n",
    "    elif code == 'lsqfit': prior['log(c)'] = gv.log([gv.gvar('2.0(2.0)'), gv.gvar('1.0(1.0)')])\n",
    "    \n",
    "    # Prior on the RBFN weights\n",
    "    prior = neural_network.network_priors(\n",
    "        prior_choice_center = { # Prior for weights\n",
    "            'lyr1': { # Only for output layer\n",
    "                'prior_type': 'ridge_regression', # Type of prior\n",
    "                'mean': 0., # Mean of zero\n",
    "                'standard_deviation': lmbda_center # Width of lambda\n",
    "            }\n",
    "        }, \n",
    "        prior_choice_bandwidth = { # Prior for weights\n",
    "            'lyr1': { # Only for output layer\n",
    "                'prior_type': 'ridge_regression', # Type of prior\n",
    "                'mean': 0., # Mean of zero\n",
    "                'standard_deviation': lmbda_bandwidth # Width of lambda\n",
    "            }\n",
    "        }, \n",
    "        prior_choice_weight = { # Prior for weights\n",
    "            'lyr2': { # Only for output layer\n",
    "                'prior_type': 'ridge_regression', # Type of prior\n",
    "                'mean': 0., # Mean of zero\n",
    "                'standard_deviation': lmbda_weight # Width of lambda\n",
    "            }\n",
    "        }, \n",
    "        prior_choice_bias = { # Prior for weights\n",
    "            'lyr2': { # Only for output layer\n",
    "                'prior_type': 'ridge_regression', # Type of prior\n",
    "                'mean': 0., # Mean of zero\n",
    "                'standard_deviation': lmbda_bias # Width of lambda\n",
    "            }\n",
    "        }, \n",
    "        prior = prior # Take in already-specified prior dictionary and modify it\n",
    "    )\n",
    "\n",
    "    \"\"\" Return prior and p0 \"\"\"\n",
    "    return prior\n",
    "\n",
    "# Get priors for both codes\n",
    "lmbda_weight, lmbda_center, lmbda_bandwidth, lmbda_bias = 1.8, 10., 5., 5.\n",
    "prior_swissfit = get_priors('swissfit', lmbda_weight, lmbda_center, lmbda_bandwidth, lmbda_bias)\n",
    "prior_lsqfit = get_priors('lsqfit', lmbda_weight, lmbda_center, lmbda_bandwidth, lmbda_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7851b457-6686-4105-948c-7166fd61d520",
   "metadata": {},
   "source": [
    "Now I fit with `SwissFit`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c86ad290-b845-4e11-a93f-54eb2aa0ebc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SwissFit: ðŸ§€\n",
      "   chi2/dof [dof] = 0.75 [29]   Q = 0.83   (Bayes) \n",
      "   chi2/dof [dof] = 0.98 [20]   Q = 0.48   (freq.) \n",
      "   AIC [k] = 39.8 [9]   logML = 132.733*\n",
      "\n",
      "Parameters*:\n",
      "     c\n",
      "             1               2.269213(38)   [2.0(2.0)]\n",
      "             2                 1.0005(27)   [1.0(1.0)]\n",
      "     lyr1.center\n",
      "             1                 -2.020(96)   [   0(10)]\n",
      "             2                  -7.13(76)   [   0(10)]\n",
      "     lyr1.bandwidth\n",
      "             1                  0.511(44)   [0.0(5.0)]\n",
      "             2                  0.074(10)   [0.0(5.0)]\n",
      "     lyr2.weight\n",
      "             1                 -0.235(33)   [0.0(1.8)]\n",
      "             2                  -2.25(57)   [0.0(1.8)]\n",
      "     lyr2.bias\n",
      "             1                0.99720(20)   [0.0(5.0)]\n",
      "\n",
      "Estimator:\n",
      "   SwissFit optimizer object\n",
      "*Laplace approximation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a function that transforms the priors into log priors to force positivity on critical parameters\n",
    "log_priors = {'c': lambda x: gv.log(x)}\n",
    "\n",
    "# Create SwissFit fit object\n",
    "fitter = fit.SwissFit(\n",
    "    udata = data, # Fit data; \"data = data\" is also acceptable - \"udata\" means \"uncorrelated\"\n",
    "    uprior = prior_swissfit, # Priors; \"prior = prior\" is also acceptable - \"uprior\" means \"uncorrelated\"\n",
    "    p0 = p0, # Starting values for parameters - chained for empirical Bayes\n",
    "    fit_fcn = fit_fcn_swissfit, # Fit function\n",
    "    prior_transformation_fcn = log_priors # Transformation of prior \"c\" to \"log(c)\"\n",
    ")\n",
    "\n",
    "# Create trust region reflective local optimizer from SciPy\n",
    "local_optimizer = scipy_least_squares.SciPyLeastSquares(fitter = fitter)\n",
    "\n",
    "# Do fit\n",
    "fitter(local_optimizer)\n",
    "print(fitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39e957e-074a-4d89-b9b9-68174add8fa4",
   "metadata": {},
   "source": [
    "And the same fit with [lsqfit](https://github.com/gplepage/lsqfit)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce310255-ed83-41e1-a711-de78dd045cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least Square Fit:\n",
      "  chi2/dof [dof] = 0.75 [29]    Q = 0.83    logGBF = 131.91\n",
      "\n",
      "Parameters:\n",
      "        log(c) 0   0.819433 (17)      [  0.7 (1.0) ]  \n",
      "               1     0.0005 (27)      [  0.0 (1.0) ]  \n",
      "   lyr1.center 0     -2.020 (96)      [     0 (10) ]  \n",
      "               1      -7.13 (76)      [     0 (10) ]  \n",
      "lyr1.bandwidth 0      0.511 (44)      [  0.0 (5.0) ]  \n",
      "               1      0.074 (10)      [  0.0 (5.0) ]  \n",
      "   lyr2.weight 0     -0.235 (33)      [  0.0 (1.8) ]  \n",
      "               1      -2.25 (57)      [  0.0 (1.8) ]  *\n",
      "     lyr2.bias 0    0.99720 (20)      [  0.0 (5.0) ]  \n",
      "----------------------------------------------------\n",
      "             c 0   2.269213 (38)      [  2.0 (2.0) ]  \n",
      "               1     1.0005 (27)      [  1.0 (1.0) ]  \n",
      "\n",
      "Settings:\n",
      "  svdcut/n = 1e-12/0    tol = (1e-08,1e-10,1e-10*)    (itns/time = 8/0.0)\n",
      "  fitter = scipy_least_squares    method = trf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Specify starting value for log(c)\n",
    "p0['log(c)'] = gv.log(p0['c'])\n",
    "\n",
    "# Do fit & print result\n",
    "fitter = lsqfit.nonlinear_fit(\n",
    "    data = data['y'],\n",
    "    fcn = fit_fcn_lsqfit,\n",
    "    prior = prior_lsqfit,\n",
    "    p0 = p0,\n",
    ")\n",
    "print(fitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a7ed2e-206f-421e-ba92-e92577a74baa",
   "metadata": {},
   "source": [
    "You may notice, depending on the version of [lsqfit](https://github.com/gplepage/lsqfit), that the marginal likelihood are slightly different. This difference stems from subtleties in how the covariance of the fit parameters is included in the marginal likelihood. In `SwissFit`, it enters the marginal likelihood through the Hessian of $\\chi^2_{\\mathrm{aug.}}$ and a negative sign is included in the log determinant to account for the fact that the Laplace-estimated covariance is the inverse of the Hessian of $\\chi^2_{\\mathrm{aug.}}$. This avoids any potential complications that can arise from taking the log determinant of the inverse, which I find can affect the estimate of the marginal likelihood. On the other hand, [lsqfit](https://github.com/gplepage/lsqfit) incorporates the covariance of the fit parameters directly. The slight difference can only be due to the inverse, and I have indeed check that this is the case. Now let's do the same thing, but without any priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "062d5dac-a412-41b2-b8dc-755187646e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SwissFit: ðŸ§€\n",
      "   chi2/dof [dof] = 0.97 [20]   Q = 0.5   (Bayes) \n",
      "   chi2/dof [dof] = 0.97 [20]   Q = 0.5   (freq.) \n",
      "   AIC [k] = 37.37 [9]   logML = 153.131*\n",
      "\n",
      "Parameters*:\n",
      "     c\n",
      "             1               2.269213(38)   [n/a]\n",
      "             2                 1.0009(27)   [n/a]\n",
      "     lyr1.center\n",
      "             1                 -2.041(98)   [n/a]\n",
      "             2                  -7.53(96)   [n/a]\n",
      "     lyr1.bandwidth\n",
      "             1                  0.499(43)   [n/a]\n",
      "             2                  0.069(11)   [n/a]\n",
      "     lyr2.weight\n",
      "             1                 -0.244(35)   [n/a]\n",
      "             2                  -2.59(85)   [n/a]\n",
      "     lyr2.bias\n",
      "             1                0.99729(22)   [n/a]\n",
      "\n",
      "Estimator:\n",
      "   SwissFit optimizer object\n",
      "*Laplace approximation\n",
      "\n",
      "Least Square Fit (no prior):\n",
      "  chi2/dof [dof] = 0.97 [20]    Q = 0.5    \n",
      "\n",
      "Parameters:\n",
      "             c 0   2.269213 (38)      [   2.26921 +- inf ]  \n",
      "               1     1.0009 (27)      [   1.00055 +- inf ]  \n",
      "   lyr1.center 0     -2.041 (98)      [  -2.02256 +- inf ]  \n",
      "               1      -7.53 (96)      [  -7.15953 +- inf ]  \n",
      "lyr1.bandwidth 0      0.499 (43)      [  0.510024 +- inf ]  \n",
      "               1      0.069 (11)      [  0.073768 +- inf ]  \n",
      "   lyr2.weight 0     -0.244 (35)      [ -0.236403 +- inf ]  \n",
      "               1      -2.59 (85)      [  -2.27198 +- inf ]  \n",
      "     lyr2.bias 0    0.99729 (22)      [  0.997205 +- inf ]  \n",
      "\n",
      "Settings:\n",
      "  svdcut/n = 0/0    tol = (1e-08,1e-10,1e-10*)    (itns/time = 14/0.0)\n",
      "  fitter = scipy_least_squares    method = trf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Remove log(c) from p0 \"\"\"\n",
    "del p0['log(c)']\n",
    "\n",
    "\"\"\" SwissFit \"\"\"\n",
    "# Create SwissFit fit object\n",
    "fitter = fit.SwissFit(\n",
    "    udata = data, # Fit data; \"data = data\" is also acceptable - \"udata\" means \"uncorrelated\"\n",
    "    p0 = p0, # Starting values for parameters - chained for empirical Bayes\n",
    "    fit_fcn = fit_fcn_swissfit, # Fit function\n",
    "    prior_transformation_fcn = log_priors # Transformation of prior \"c\" to \"log(c)\"\n",
    ")\n",
    "\n",
    "# Create trust region reflective local optimizer from SciPy\n",
    "local_optimizer = scipy_least_squares.SciPyLeastSquares(fitter = fitter)\n",
    "\n",
    "# Do fit\n",
    "fitter(local_optimizer)\n",
    "print(fitter)\n",
    "\n",
    "\"\"\" lsqfit \"\"\"\n",
    "# Do fit & print result\n",
    "fitter = lsqfit.nonlinear_fit(\n",
    "    udata = data['y'],\n",
    "    fcn = fit_fcn_lsqfit,\n",
    "    p0 = p0\n",
    ")\n",
    "print(fitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f0bdbd-1835-4e08-ade6-79198e16587c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
